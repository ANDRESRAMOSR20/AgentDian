from langchain_ollama import ChatOllama
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import RedisChatMessageHistory
from typing import List, Union, TypedDict
import asyncio

# ğŸ“Œ 1ï¸âƒ£ DefiniciÃ³n del Estado de ConversaciÃ³n
CustomState = TypedDict("CustomState", {"messages": List[Union[HumanMessage, AIMessage]], "history": RedisChatMessageHistory})

# ğŸ“Œ 2ï¸âƒ£ ConfiguraciÃ³n del Modelo LLM con herramientas
print("ğŸ”§ Inicializando modelo LLM...")
try:
    llm = ChatOllama(model="hf.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF:IQ4_NL")
    print("âœ… Modelo LLM cargado correctamente.")
except Exception as e:
    print(f"âŒ Error al inicializar el modelo LLM: {e}")

# ğŸ“Œ 3ï¸âƒ£ FunciÃ³n para gestionar consultas y respuestas
def query_or_respond(state: CustomState):
    """Recupera historial antes de responder y almacena nuevas interacciones."""
    
    if "messages" not in state or not isinstance(state["messages"], list):
        return {"messages": [AIMessage(content="No hay mensajes en la conversaciÃ³n.")]}

    if "history" not in state:
        state["history"] = RedisChatMessageHistory(session_id="default_session")

    user_message = state["messages"][-1]
    print(f"ğŸ“ Ãšltimo mensaje del usuario: {user_message.content}")

    # ğŸ”¹ Recuperar historial antes de responder
    try:
        stored_messages = asyncio.run(state["history"].aget_messages())
    except Exception as e:
        print(f"âš ï¸ Error al recuperar historial: {e}")
        stored_messages = []

    # ğŸ”¹ Agregar el nuevo mensaje al historial
    state["history"].add_user_message(user_message.content)

    # ğŸ”¹ Pasar historial + mensaje al modelo
    all_messages = stored_messages + [user_message]
    response = llm.invoke(all_messages)  # Ahora el modelo recibe contexto

    response_content = response.content if isinstance(response, AIMessage) else response
    state["history"].add_ai_message(response_content)

    return {"messages": [AIMessage(content=response_content)]}


# ğŸ“Œ 4ï¸âƒ£ FunciÃ³n de decisiÃ³n para usar herramientas o generar respuesta directa
def tools_condition(state: CustomState):
    """Determina si se debe usar la herramienta de recuperaciÃ³n."""
    if not state["messages"]:  # Evita IndexError si la lista estÃ¡ vacÃ­a
        return END
    last_message = state["messages"][-1].content.lower()
    return "tools" if any(keyword in last_message for keyword in ["buscar", "recuperar", "informaciÃ³n"]) else END

# ğŸ“Œ 5ï¸âƒ£ DefiniciÃ³n del Nodo de Herramientas para RecuperaciÃ³n
from architecture_rag import retrieve
tools = ToolNode([retrieve])

# ğŸ“Œ 6ï¸âƒ£ ConstrucciÃ³n del Grafo de ConversaciÃ³n
workflow = StateGraph(CustomState)

# ConfiguraciÃ³n del flujo
workflow.add_node("query_or_respond", query_or_respond)
workflow.add_node("tools", tools)

workflow.set_entry_point("query_or_respond")
workflow.add_conditional_edges("query_or_respond", tools_condition, {"tools": "tools", END: END})

# ğŸ”¥ CompilaciÃ³n y ejecuciÃ³n del grafo
try:
    app = workflow.compile()
    print("âœ… Grafo de conversaciÃ³n compilado correctamente.")
except Exception as e:
    print(f"âŒ Error al compilar el grafo de conversaciÃ³n: {e}")

# ğŸ“Œ ğŸ” Prueba inicial del modelo
try:
    test_message = [HumanMessage(content="Hola, Â¿CuÃ¡l es el nombre del presidente de Estados Unidos?")]
    test_response = llm.invoke(test_message)
    print(f"âœ… Prueba exitosa: {test_response.content}")
except Exception as e:
    print(f"âŒ Error en la prueba del modelo: {e}")
