from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from typing import List, Union, TypedDict
from langchain_community.chat_message_histories import RedisChatMessageHistory
from langgraph.graph import StateGraph, END

CustomState = TypedDict("CustomState", {"messages": List[Union[HumanMessage, AIMessage]], "history": RedisChatMessageHistory})


# ğŸ“Œ 1ï¸âƒ£ GeneraciÃ³n de Respuesta Basada en RecuperaciÃ³n
def generate(state: CustomState):
    """Genera una respuesta utilizando el contexto recuperado y el historial de conversaciÃ³n."""

    # ğŸ”¹ Obtiene los mensajes de herramientas recientes (recuperaciÃ³n)
    tool_messages = [
        message for message in reversed(state["messages"])
        if isinstance(message, AIMessage) and hasattr(message, "tool_calls") and message.tool_calls
    ]

    if not tool_messages:
        docs_content = "No se encontrÃ³ informaciÃ³n relevante en la base de datos."
    else:
        docs_content = "\n\n".join(set(msg.content for msg in tool_messages if msg.content))

    # ğŸ”¹ Mensaje del sistema con el contexto recuperado
    system_message_content = (
        "Usted es un asistente para tareas de respuesta a preguntas. "
        "Utilice los siguientes elementos de contexto recuperados para responder "
        "las preguntas. si no conoces la resputa, di que "
        "no la sabes. Utiliza cinco frases como maximo para"
        "responder concisamente."
        "\n\n"
        f"{docs_content}"
    )

    # ğŸ”¹ Obtiene el historial de conversaciÃ³n
    historical_messages = state["history"].messages

    # ğŸ”¹ Obtiene los mensajes recientes de usuario y sistema
    conversation_messages = [
        msg for msg in state["messages"]
        if isinstance(msg, (HumanMessage, SystemMessage)) or (isinstance(msg, AIMessage) and not getattr(msg, "tool_calls", None))
    ]

    # ğŸ”¹ Construye el prompt final para el LLM
    full_prompt = (
        [SystemMessage(content=system_message_content)]
        + historical_messages  # Historial completo
        + conversation_messages  # Mensaje actual
    )

    # ğŸ”¹ Genera la respuesta con el modelo de lenguaje
    try:
        print("ğŸ¤– Generando respuesta con el modelo...")
        response = llm_with_tools.invoke(full_prompt)  # âœ… Se corrigiÃ³ la variable usada en invoke()

        if not response or not hasattr(response, "content") or not response.content.strip():
            print("âŒ La respuesta generada estÃ¡ vacÃ­a o no es vÃ¡lida.")
            response = AIMessage(content="Lo siento, no pude encontrar una respuesta adecuada.")

        print(f"âœ… Respuesta generada: {response.content}")

    except Exception as e:
        print(f"âŒ Error al generar la respuesta: {e}")
        response = AIMessage(content="Error al generar la respuesta.")

    # ğŸ”¹ Agrega la respuesta al historial evitando duplicados
    previous_messages = {msg.content for msg in state["history"].messages}

    if response.content in previous_messages:
        print("âš ï¸ Respuesta duplicada detectada, no se almacena.")
    else:
        state["history"].add_ai_message(response.content)
        print("âœ… Respuesta guardada en historial de Redis.")

    return {"messages": [response]}


# ğŸ“Œ 2ï¸âƒ£ ConfiguraciÃ³n del Grafo de EjecuciÃ³n
from confg import query_or_respond, tools, tools_condition

graph_builder = StateGraph(CustomState)
graph_builder.add_node("query_or_respond", query_or_respond)  # Decide si recuperar o responder
graph_builder.add_node("tools", tools)  # Ejecuta la recuperaciÃ³n si es necesario
graph_builder.add_node("generate", generate)  # Genera la respuesta final

# ğŸ”¹ Punto de Entrada del Grafo
graph_builder.set_entry_point("query_or_respond")

# ğŸ”¹ DefiniciÃ³n de Condiciones para Decidir si Buscar InformaciÃ³n
graph_builder.add_conditional_edges(
    "query_or_respond",
    tools_condition,
    {END: END, "tools": "tools"},  # Si se requiere, pasa por la herramienta de recuperaciÃ³n
)

# ğŸ”¹ Flujo del Grafo: De `tools` a `generate` y luego Finaliza
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)

# ğŸ“Œ 3ï¸âƒ£ CompilaciÃ³n y EjecuciÃ³n del Grafo con Mejoras
graph = graph_builder.compile()

# ğŸ“Œ 4ï¸âƒ£ EjecuciÃ³n con una Consulta de Prueba
input_message = "Â¿Me gustarÃ­a saber quÃ© ha sucedido con los presidentes que han servido?"

# Inicializar la historia de chat con Redis
message_history = RedisChatMessageHistory(session_id="test_session")

# Estado inicial con mensajes y historial
initial_state = {
    "messages": [HumanMessage(content=input_message)],
    "history": message_history
}

# ğŸš€ Iniciando ejecuciÃ³n del grafo con mejoras...
print("\nğŸš€ Iniciando ejecuciÃ³n del grafo...")

# âœ… Limpiar historial antes de procesar la nueva consulta
print("ğŸ—‘ï¸ Limpiando historial en Redis...")
message_history.clear()

for step in graph.stream(
    initial_state,
    stream_mode="values",
):
    if step["messages"]:
        last_message = step["messages"][-1]
    else:
        print("âš ï¸ No hay mensajes en este paso del grafo. Se genera un mensaje predeterminado.")
        last_message = AIMessage(content="No se pudo generar una respuesta en este paso.")

    # âœ… Verificar si la respuesta es vÃ¡lida antes de guardarla
    if not last_message or not last_message.content.strip():
        print("âŒ Respuesta vacÃ­a o invÃ¡lida. Generando respuesta alternativa.")
        last_message = AIMessage(content="Lo siento, no encontrÃ© informaciÃ³n relevante.")

    # ğŸ“Œ Evitar respuestas duplicadas o que repitan el mensaje del usuario
    try:
        previous_messages = {msg.content for msg in message_history.messages}
    except AttributeError:
        print("âš ï¸ No se pudo obtener el historial de Redis, se usarÃ¡ un historial vacÃ­o.")
        previous_messages = set()    
        
    if last_message.content in previous_messages or last_message.content == input_message:
        print("âš ï¸ Respuesta duplicada o repeticiÃ³n del mensaje del usuario, no se almacena.")
    else:
        message_history.add_ai_message(last_message.content)
        print("âœ… Respuesta guardada en historial de Redis.")
    
    # ğŸ“Œ Mostrar la respuesta generada
    last_message.pretty_print()

# ğŸ“Œ 5ï¸âƒ£ VisualizaciÃ³n del Historial de Mensajes Limpio
print("\nğŸ” Historial de ConversaciÃ³n:")
for message in message_history.messages:
    print(f"{message.type}: {message.content}")
